{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demystifying the mathematics behind PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know PCA and we all love PCA. Our friend that helps us deal with the curse of dimensionality. Everyone has probably used [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). I thought I knew PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until I was asked to explain the mathematics behind PCA in an interview and all I could murmur was that it somehow maximizes the variance of the along the new dimensions. The interviewer was even kind enough to throw me a hint about projections. To my chagrin, I couldn't figure it out even then and had to admit I was stumped. To rub salt in my wounds, I knew that I was taught the mathematics behind PCA during the first year of my Master's course. So here's a post to make sure that doesn't happen to you, dear reader, and hopefully me as well :smile:.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: This post will be long and mathematical as the title suggests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA stands for Principal Component Analysis. If you have multidimensional data that's giving you a hard time when you try to train a model on it, PCA could be the answer. You could also visualize high dimensional data using PCA, which is done often in [NLP](https://necromuralist.github.io/Neurotic-Networking/posts/nlp/pca-dimensionality-reduction-and-word-vectors/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's no understanding without doing, so let's write some code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub><sup>(hidden latex commands)</sup></sub>\n",
    "$\\renewcommand{\\Cov}{\\mathrm{Cov}}$\n",
    "$\\renewcommand{\\var}{\\mathrm{var}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of data, that we will call $x$. $x$ is $D$ dimensional, hence $x \\in \\mathbb{R}^D$ (just a fancy way of saying that our points are real numbers in D dimensions). Our goal to represent $x$ in less than $D$ dimensions. This is by no means going to be a perfect transition. Say we project down from $D$ dimensions to $M$ dimensions. By definition, $M<D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# say D=10, i.e we have 10 dimensional data.\n",
    "x = np.random.rand(10,10)\n",
    "# for the geeks amongst us this is from a continuous uniform distrbution from [0, 1)\n",
    "x.shape\n",
    "# so we have 10 data points each of which have 10 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical approach is to maximize the variance of the $M$ dimensional data in such a way that it captures as much of the variance of the $D$ dimensional data as possible. Let's call $z$ the co-ordinates of the projection of $x$ into the lower M dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Harold Hotelling](https://en.wikipedia.org/wiki/Harold_Hotelling) is credited with coming up with the idea of minimizing the variance in 1933. Mr.Hotelling was quite the madlad, since he also came up with the T-square distribution amongst other laws, lemmas and rules. He wrote presciently in his original paper, 90 years ago: \n",
    ">\"It is natural to ask whether some more fundamental set of independent variables exists, perhaps fewer in number than the $x$'s, which determine the values the $x$'s will take.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gentle math:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce the mean. This is our \"average\" friend.\n",
    "$$\\bar{x}=\\frac{1}{n}\\sum_{i}^{n}x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45763203983322837"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in numpy if we don't define the axis, it will compute the mean of all the 100 elements that we entered\n",
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58607683, 0.35993049, 0.4649175 , 0.53540096, 0.51442973,\n",
       "       0.50757007, 0.42450736, 0.25607604, 0.50758928, 0.41982214])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we're dealing with components, we are more interested in the columwise or featurewise mean\n",
    "x.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5860768282330243\n",
      "0.359930493813573\n",
      "0.46491749778618263\n",
      "0.5354009642518928\n",
      "0.5144297337485908\n",
      "0.5075700667940669\n",
      "0.42450736094776953\n",
      "0.2560760356268859\n",
      "0.5075892818724383\n",
      "0.4198221352578594\n"
     ]
    }
   ],
   "source": [
    "# if you're paranoid and you'd like to check the values, here you go:\n",
    "for i in range(0,10):\n",
    "    print(x[:,i].sum()/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard deviation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation is just how much each point deviates from the mean. It measures how spread out our data is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma=\\sqrt{\\frac{1}{n}\\sum_{i}^{n}(x_i - \\bar{x})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29150845, 0.25559572, 0.30291139, 0.24915131, 0.29970886,\n",
       "       0.26688689, 0.24686484, 0.25656166, 0.27335956, 0.275878  ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# say we have some different data:\n",
    "y = np.array([1]*6)\n",
    "z = np.array([0]*3+[1]*3)\n",
    "w = np.arange(1,7)\n",
    "arrays = [y, z, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n",
      "3.5\n"
     ]
    }
   ],
   "source": [
    "# we can check their means:\n",
    "for _ in arrays:\n",
    "    print(_.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5\n",
      "1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "# we can also check their standard deviation:\n",
    "for _ in arrays:\n",
    "    print(_.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5\n",
      "1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "# paranoia tax\n",
    "for _ in arrays:\n",
    "    current_mean = _.mean()\n",
    "    print(np.sqrt(np.sum((_ - current_mean)**2)/len(_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense when you consider that our array y had only one value and has a standard deviation of 0. Our array z was slightly better spread out, but still it only had 2 unique values each occuring thrice. Our array w was the most numerically diverse of y, z and w, and hence it had the highest standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance is simply the square of the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\renewcommand{\\var}{\\mathrm{var}}$\n",
    "$$ \\var(x) = \\sigma^2=\\frac{1}{n}\\sum_{i}^{n}(x_i - \\bar{x})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08497718, 0.06532917, 0.09175531, 0.06207638, 0.0898254 ,\n",
       "       0.07122861, 0.06094225, 0.06582388, 0.07472545, 0.07610867])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.var(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.25\n",
      "2.9166666666666665\n"
     ]
    }
   ],
   "source": [
    "# consider the 1d line data again\n",
    "for _ in arrays:\n",
    "    print(_.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the measures we looked at so far were dealing with only one dimension. Notice that while we have just one number to describe the mean, $\\sigma$ and $\\sigma^2$ for our one liner data (y,z,w), since they contain only one dimension; we had to specify an axis for $x$, since $x$ is a collection of 10 data points with 10 dimensions each. Also notice that when dealing with $x$, all 3 measures had 10 values, that is as many values as the number of dimensions in $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out the interactions between the dimensions and how they vary depending on each other, we introduce the aptly named covariance term. The covariance is always measured between two dimensions. Notice the similarities between the variance and the covariance below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\renewcommand{\\var}{\\mathrm{var}}$\n",
    "$$ \\var(x) \\frac{1}{n}\\sum_{i}^{n}(x_i - \\bar{x})(x_i - \\bar{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\renewcommand{\\Cov}{\\mathrm{Cov}}$\n",
    "$$ \\Cov(x,y) = \\frac{1}{n}\\sum_{i}^{n}(x_i - \\bar{x})(y_i - \\bar{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be nice to see the covariance of variable on a real dataset rather than the random data we have here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy data from the wine sklearn dataset\n",
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()[\"data\"]\n",
    "data.shape\n",
    "# we have 13 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that covariance can only capture the variance between 2 variables. Additionally, we also know that the covarince of a feature with itself is the variance. Perhaps $\\Cov(a,b) \\neq \\Cov(b,a)$. So, as far as a count of covariances goes, in this case with 13 features, we will have a 13x13 matrix, because of the reasons we just mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 13)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features are in columns, hence rowvar is false\n",
    "np.cov(data, rowvar=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.59062328e-01,  8.56113090e-02,  4.71151590e-02,\n",
       "        -8.41092903e-01,  3.13987812e+00,  1.46887218e-01,\n",
       "         1.92033222e-01, -1.57542595e-02,  6.35175205e-02,\n",
       "         1.02828254e+00, -1.33134432e-02,  4.16978226e-02,\n",
       "         1.64567185e+02],\n",
       "       [ 8.56113090e-02,  1.24801540e+00,  5.02770393e-02,\n",
       "         1.07633171e+00, -8.70779534e-01, -2.34337723e-01,\n",
       "        -4.58630366e-01,  4.07333619e-02, -1.41146982e-01,\n",
       "         6.44838183e-01, -1.43325638e-01, -2.92447483e-01,\n",
       "        -6.75488666e+01],\n",
       "       [ 4.71151590e-02,  5.02770393e-02,  7.52646353e-02,\n",
       "         4.06208278e-01,  1.12293658e+00,  2.21455913e-02,\n",
       "         3.15347299e-02,  6.35847140e-03,  1.51557799e-03,\n",
       "         1.64654327e-01, -4.68215451e-03,  7.61835841e-04,\n",
       "         1.93197391e+01],\n",
       "       [-8.41092903e-01,  1.07633171e+00,  4.06208278e-01,\n",
       "         1.11526862e+01, -3.97476036e+00, -6.71149146e-01,\n",
       "        -1.17208281e+00,  1.50421856e-01, -3.77176220e-01,\n",
       "         1.45024186e-01, -2.09118054e-01, -6.56234368e-01,\n",
       "        -4.63355345e+02],\n",
       "       [ 3.13987812e+00, -8.70779534e-01,  1.12293658e+00,\n",
       "        -3.97476036e+00,  2.03989335e+02,  1.91646988e+00,\n",
       "         2.79308703e+00, -4.55563385e-01,  1.93283248e+00,\n",
       "         6.62052061e+00,  1.80851266e-01,  6.69308068e-01,\n",
       "         1.76915870e+03],\n",
       "       [ 1.46887218e-01, -2.34337723e-01,  2.21455913e-02,\n",
       "        -6.71149146e-01,  1.91646988e+00,  3.91689535e-01,\n",
       "         5.40470422e-01, -3.50451247e-02,  2.19373345e-01,\n",
       "        -7.99975192e-02,  6.20388758e-02,  3.11021278e-01,\n",
       "         9.81710573e+01],\n",
       "       [ 1.92033222e-01, -4.58630366e-01,  3.15347299e-02,\n",
       "        -1.17208281e+00,  2.79308703e+00,  5.40470422e-01,\n",
       "         9.97718673e-01, -6.68669999e-02,  3.73147553e-01,\n",
       "        -3.99168626e-01,  1.24081969e-01,  5.58262255e-01,\n",
       "         1.55447492e+02],\n",
       "       [-1.57542595e-02,  4.07333619e-02,  6.35847140e-03,\n",
       "         1.50421856e-01, -4.55563385e-01, -3.50451247e-02,\n",
       "        -6.68669999e-02,  1.54886339e-02, -2.60598680e-02,\n",
       "         4.01205097e-02, -7.47117692e-03, -4.44692440e-02,\n",
       "        -1.22035863e+01],\n",
       "       [ 6.35175205e-02, -1.41146982e-01,  1.51557799e-03,\n",
       "        -3.77176220e-01,  1.93283248e+00,  2.19373345e-01,\n",
       "         3.73147553e-01, -2.60598680e-02,  3.27594668e-01,\n",
       "        -3.35039177e-02,  3.86645655e-02,  2.10932940e-01,\n",
       "         5.95543338e+01],\n",
       "       [ 1.02828254e+00,  6.44838183e-01,  1.64654327e-01,\n",
       "         1.45024186e-01,  6.62052061e+00, -7.99975192e-02,\n",
       "        -3.99168626e-01,  4.01205097e-02, -3.35039177e-02,\n",
       "         5.37444938e+00, -2.76505801e-01, -7.05812576e-01,\n",
       "         2.30767480e+02],\n",
       "       [-1.33134432e-02, -1.43325638e-01, -4.68215451e-03,\n",
       "        -2.09118054e-01,  1.80851266e-01,  6.20388758e-02,\n",
       "         1.24081969e-01, -7.47117692e-03,  3.86645655e-02,\n",
       "        -2.76505801e-01,  5.22449607e-02,  9.17662439e-02,\n",
       "         1.70002234e+01],\n",
       "       [ 4.16978226e-02, -2.92447483e-01,  7.61835841e-04,\n",
       "        -6.56234368e-01,  6.69308068e-01,  3.11021278e-01,\n",
       "         5.58262255e-01, -4.44692440e-02,  2.10932940e-01,\n",
       "        -7.05812576e-01,  9.17662439e-02,  5.04086409e-01,\n",
       "         6.99275256e+01],\n",
       "       [ 1.64567185e+02, -6.75488666e+01,  1.93197391e+01,\n",
       "        -4.63355345e+02,  1.76915870e+03,  9.81710573e+01,\n",
       "         1.55447492e+02, -1.22035863e+01,  5.95543338e+01,\n",
       "         2.30767480e+02,  1.70002234e+01,  6.99275256e+01,\n",
       "         9.91667174e+04]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(data, rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive covariance values between two features indicate that as one feature increases the other also increases. Negative values indicate an increase-decrease relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting to get serious math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    When trying to do PCA, we can follow one of two approaches:\n",
    "        1. Minimize the reconstruction error\n",
    "        2. Maximize the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "    1. Slides by Pascal Vincet and explanations by Ioannis Mitliagkas in the course IFT630 - Fundamentals of Machine Learning taught at the University of Montreal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
